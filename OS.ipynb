{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OS.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"ra0NLpewnhkI","executionInfo":{"status":"error","timestamp":1614398034387,"user_tz":-330,"elapsed":402,"user":{"displayName":"Narendra G O 19BCE1082","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVpvJB7f3tMwwTeulX-17wxzlVHpAgTgouqitC=s64","userId":"13443054282008759577"}},"outputId":"3b137700-bead-4ff4-fadc-679e161eda96"},"source":["from urllib.request import Request, urlopen, URLError, urljoin\n","from urllib.parse import urlparse\n","import time\n","import threading\n","import queue\n","from bs4 import BeautifulSoup\n","import ssl\n","\n","class Crawler(threading.Thread):\n","    def __init__(self,base_url, links_to_crawl,have_visited, error_links,url_lock):\n","       \n","        threading.Thread.__init__(self)\n","        print(f\"Web Crawler worker {threading.current_thread()} has Started\")\n","        self.base_url = base_url\n","        self.links_to_crawl = links_to_crawl\n","        self.have_visited = have_visited\n","        self.error_links = error_links\n","        self.url_lock = url_lock\n","\n","    def run(self):\n","        # we create a ssl context so that our script can crawl\n","        # the https sties with ssl_handshake.\n","\n","        #Create a SSLContext object with default settings.\n","        my_ssl = ssl.create_default_context()\n","\n","        # by default when creating a default ssl context and making an handshake\n","        # we verify the hostname with the certificate but our objective is to crawl\n","        # the webpage so we will not be checking the validity of the cerfificate.\n","        my_ssl.check_hostname = False\n","\n","        # in this case we are not verifying the certificate and any \n","        # certificate is accepted in this mode.\n","        my_ssl.verify_mode = ssl.CERT_NONE\n","\n","        # we are defining an infinite while loop so that all the links in our\n","        # queue are processed.\n","\n","        while True:\n","\n","            # In this part of the code we create a global lock on our queue of \n","            # links so that no two threads can access the queue at same time\n","            self.url_lock.acquire()\n","            print(f\"Queue Size: {self.links_to_crawl.qsize()}\")\n","            link = self.links_to_crawl.get()\n","            self.url_lock.release()\n","\n","            # if the link is None the queue is exhausted or the threads are yet\n","            # process the links.\n","\n","            if link is None:\n","                break\n","            \n","            # if The link is already visited we break the execution.\n","            if link in self.have_visited:\n","                print(f\"The link {link} is already visited\")\n","                break\n","\n","            try:\n","                # This method constructs a full \"absolute\" URL by combining the\n","                # base url with other url. this uses components of the base URL, \n","                # in particular the addressing scheme, the network \n","                # location and  the path, to provide missing components \n","                # in the relative URL.\n","                # in short we repair our relative url if it is broken.\n","                link = urljoin(self.base_url,link)\n","\n","                # we use the header parameter to \"spoof\" the \"User-Agent\" header\n","                # value which is used by the browser to identify itself. This is\n","                # because some servers will only allow the connection if  it comes\n","                # from a verified browser. In this case we are using FireFox header. \n","                req = Request(link, headers= {'User-Agent': 'Mozilla/5.0'})\n","\n","                # we are opening the url using a ssl handshake.\n","                response = urlopen(req, context=my_ssl)\n","\n","                print(f\"The URL {response.geturl()} crawled with \\\n","                      status {response.getcode()}\")\n","\n","                # this returns the html representation of the webpage\n","                soup = BeautifulSoup(response.read(),\"html.parser\")\n","\n","                # in this case we are finding all the links in the page.\n","                for a_tag in soup.find_all('a'):\n","                    # we are checking of the link is already visited and (network location part) is our\n","                    # base url itself.\n","                    if (a_tag.get(\"href\") not in self.have_visited) and (urlparse(link).netloc == \"www.python.org\"):\n","                        self.links_to_crawl.put(a_tag.get(\"href\"))\n","                    \n","                    else:\n","                        print(f\"The link {a_tag.get('href')} is already visited or is not part \\\n","                        of the website\")\n","\n","                print(f\"Adding {link} to the crawled list\")\n","                self.have_visited.add(link)\n","\n","            except URLError as e:\n","                print(f\"URL {link} threw this error {e.reason} while trying to parse\")\n","\n","                self.error_links.append(link)\n","\n","            finally:\n","                self.links_to_crawl.task_done()\n","\n","print(\"The Crawler is started\")\n","base_url = input(\"Please Enter Website to Crawl > \")\n","number_of_threads = input(\"Please Enter number of Threads > \")\n","\n","links_to_crawl = queue.Queue()\n","url_lock = threading.Lock()\n","links_to_crawl.put(base_url)\n","\n","have_visited = set()\n","crawler_threads = []\n","error_links = []\n","#base_url, links_to_crawl,have_visited, error_links,url_lock\n","for i in range(int(number_of_threads)):\n","    crawler = Crawler(base_url = base_url, \n","                      links_to_crawl= links_to_crawl, \n","                      have_visited= have_visited,\n","                      error_links= error_links,\n","                      url_lock=url_lock)\n","    \n","    crawler.start()\n","    crawler_threads.append(crawler)\n","\n","\n","for crawler in crawler_threads:\n","    crawler.join()\n","\n","\n","\n","print(f\"Total Number of pages visited are {len(have_visited)}\")\n","print(f\"Total Number of Errornous links: {len(error_links)}\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-41b2989b624a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcrawler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrawler_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mcrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}